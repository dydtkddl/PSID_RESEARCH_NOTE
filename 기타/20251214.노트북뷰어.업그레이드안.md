
<!-- 파일명: 20251214.노트북뷰어.업그레이드안.md -->

---
date: 2025-12-14
project: notebook_viewer
status: plan
tags: [fastapi, markdown, llm, gemini, chatgpt, pdf, marker, translation, annotation]
---

# 20251214.노트북뷰어.업그레이드안

## 0) 목표

- (기능 A) **PDF 업로드 → Marker 기반 MD 변환 → 문맥 chunking → LLM 번역/정리 → 번역본 MD 자동 저장**까지 원클릭 자동화
- (기능 B) **문서에서 드래그한 구간에 대해 질문 → LLM 줄글 답변 → 해당 구간 아래에 blockquote 노트로 삽입/저장**

---

## 1) 기능 A: PDF → MD → Chunk → 번역/정리 → 통합 저장

### 1.1 최종 산출물(저장 형태)

- 저장 위치(기본): `ROOT_DIR/<프로젝트>/papers/<slug>/`
- 폴더 구조 예시:
  - `.../papers/20251214_paper_title_slug/`
    - `source.pdf`
    - `marker.md`                (marker 변환 결과, 원문 기반)
    - `chunks.jsonl`             (chunk 단위 원문/메타)
    - `translated.md`            (최종 번역본/정리본)
    - `prompts/`                 (chunk별 prompt/response 로그)
      - `0001.prompt.txt`
      - `0001.response.txt`
    - `job.json`                 (진행상태/모델/시간/오류)
    - `assets/`                  (marker가 추출한 이미지/표 등 필요 시)
- `translated.md`는 최소 구성:
  - 문서 제목/메타(파일명, 업로드 시각, 모델/버전)
  - 목차(헤딩 기반)
  - 본문(번역 + 수식/코드블록 유지)
  - (옵션) “용어 통일 표”, “요약”, “핵심 질문” 섹션

### 1.2 파이프라인(서버 실행 순서)

1) **PDF 업로드**
- endpoint: `POST /research/api/papers/upload`
- 저장: `papers/<slug>/source.pdf`
- job 레코드 생성: `job_id`, `status=queued`

2) **Marker 변환 (PDF → MD)**
- 실행: marker를 subprocess 또는 python API로 호출
- 결과: `marker.md` (+ 이미지/도표 있으면 `assets/`)

3) **MD 정리(전처리)**
- 고정 규칙:
  - 연속 공백/개행 정리(현재 `normalize_markdown_text` 재사용 가능)
  - 너무 긴 라인/깨진 하이픈(-) 줄바꿈 완화
  - 수식 구문 보존: `\( \)`, `\[ \]`, `$ $`, `$$ $$`

4) **Chunking (문맥 기반 분할)**
- 기본 전략(우선순위):
  - 헤딩(`^#{1,6}\s`) 기준으로 1차 분할
  - 헤딩이 부족하면 문단/빈줄 기준 분할
  - chunk 길이 제한: `max_tokens` 또는 `max_chars` (예: 2,000~4,000 토큰)
- chunk 메타:
  - `chunk_id`, `heading_path`, `start_idx/end_idx`(텍스트 오프셋), `sha1`
- 저장: `chunks.jsonl` (1줄=1chunk JSON)

5) **LLM 번역 + MD 재구성(Chunk 단위)**
- provider 선택:
  - `gemini` / `openai` (config에서 default + per-job override)
- 프롬프트 규칙(권장):
  - “번역은 한국어, 문장 자연스럽게, 원문 구조(헤딩/리스트/표) 유지”
  - 수식/코드/인용 블록은 절대 훼손 금지
  - 용어 통일: 첫 등장에 (영문/약어) 병기
- 출력: “번역된 chunk MD” (+ 옵션: ‘핵심 요약 bullet 3개’)

6) **통합/후처리**
- chunk 순서대로 합쳐 `translated.md` 생성
- 후처리:
  - 목차 생성(옵션)
  - 잘린 표/리스트 정렬
  - 이미지 경로 정규화(필요 시 `/research/media/...` 규칙과 호환)

7) **저장 완료 + UI 노출**
- viewer에서 바로 열람 가능
- project 목록/최근 문서/검색에도 자연스럽게 인덱싱

### 1.3 API/화면 설계

- API
  - `POST /research/api/papers/upload`
    - form-data: `project_rel_path`, `pdf`
    - resp: `{ job_id, paper_rel_dir, source_rel_path }`
  - `POST /research/api/papers/{job_id}/start`
    - resp: `{ ok: true }`
  - `GET /research/api/papers/{job_id}/status`
    - resp: `{ status, step, progress, message, out_rel_path, error }`
  - `GET /research/api/papers/{job_id}/result`
    - resp: `{ translated_rel_path }`
- UI (추천 2안 중 택1, 구현 난이도 낮은 순)
  - **안1: “새 문서 생성(new.html)”에 PDF 업로드 섹션 추가**
  - **안2: viewer.html에 “PDF→번역본 생성” 버튼(현재 PDF 미리보기 화면에서)**

### 1.4 서버 구조 리팩터링(권장)

- 파일 분리(지금 main.py가 너무 비대해질 가능성 큼)
  - `services/llm_clients.py` : OpenAI/Gemini 호출 래퍼(키 로테이션, retry, rate limit)
  - `services/pdf_marker.py`  : marker 실행/결과 수집
  - `services/chunker.py`     : 헤딩/문단 기반 chunker + token/char limiter
  - `services/paper_jobs.py`  : job 생성/상태 업데이트/로그/결과 경로 관리
  - `routes/papers.py`        : 업로드/상태/결과 API 라우터
- 저장소(간단/안전 우선)
  - MVP: `jobs.json` + file lock
  - 안정화: `sqlite (jobs.db)`로 전환 (동시 요청/재시작 복구 유리)

### 1.5 로깅/재현성

- job 폴더에 항상 남김:
  - `job.json` (step별 시작/종료 시간, 모델, 에러)
  - `prompts/xxxx.prompt.txt`, `prompts/xxxx.response.txt`
- 서버 로그:
  - 기존 `notebook_viewer.log` 유지
  - 장기 작업은 step별 `logger.info("job=%s step=%s ...")` 형태로 추적

---

## 2) 기능 B: 드래그 구간 Q&A → blockquote 노트 삽입/저장

### 2.1 UX 플로우(사용자 동작)

1) viewer에서 **텍스트 드래그**
2) 선택 영역 근처에 작은 플로팅 버튼 표시: `❓질문`
3) 버튼 클릭 → 모달 오픈
   - 입력: 질문(짧은 줄글)
   - 표시: 선택 텍스트(읽기 전용)
   - 버튼: `답변 생성`
4) 답변 표시(줄글)
5) 버튼: `노트로 삽입 + 저장`
6) 저장 후 viewer 새로고침 → **선택 구간 “아래”에** 아래 형식으로 자동 삽입

### 2.2 삽입 포맷(권장)

- 기본 blockquote + 타임스탬프 + 질문/답변
- 예시:

> **GPT 노트 (2025-12-14 01:23)**
> **Q.** (사용자 질문)
> **A.** (모델 답변 줄글)

- 중복 방지용 마커(HTML comment, 렌더에는 안 보임):
  - `<!-- nv-anno:sha1=<hash> -->`

### 2.3 “어디에 꽂을지” 결정 로직(서버)

- 입력: `rel_path`, `selected_text`, `question`, (옵션) `context_hint`
- 추천 매칭 방식(안전한 순서):
  1) **정확 매칭**: 원문 md에서 `selected_text`를 찾고, 그 구간이 포함된 “문단 끝” 다음에 삽입
  2) 실패 시 **완화 매칭**:
     - 공백/개행 normalize 후 검색
     - 너무 길면 앞/뒤 120자씩 잘라서 검색
  3) 다중 매칭이면:
     - 가장 먼저 등장하는 위치 + 주변 문맥 similarity(간단히 “앞뒤 50자” 비교)로 최적 선택
- 삽입 후:
  - 기존 `save_file` 로직처럼 `.history` 백업 생성
  - `normalize_markdown_text()` 적용(맑게 유지)

### 2.4 API/화면 설계

- API
  - `POST /research/api/annotate/ask`
    - body: `{ provider?, model?, selected_text, question }`
    - resp: `{ answer_text, md_blockquote }`
  - `POST /research/api/annotate/apply`
    - body: `{ rel_path, selected_text, md_blockquote, anchor_hash }`
    - resp: `{ ok, new_rel_path? }`
- UI 구현 포인트(viewer.html)
  - `mouseup` 이벤트로 selection 감지
  - selection 길이 임계값(예: 20자 이상)에서만 플로팅 버튼 표시
  - 모달은 `static/style.css`로 간단 구현(또는 최소 HTML로 overlay)
  - 저장 성공 시 `window.location.reload()` 또는 해당 위치로 스크롤

---

## 3) 공통 설정(config.json) 확장안

> 현재 config.json은 auth만 사용 중. 아래처럼 확장(하위호환 유지)

```json
{
  "auth": { "username": "admin", "password": "admin" },
  "llm": {
    "default_provider": "gemini",
    "providers": {
      "openai": { "api_keys": ["..."], "model": "gpt-4.1-mini" },
      "gemini": { "api_keys": ["..."], "model": "gemini-2.0-flash" }
    },
    "translation": {
      "max_chunk_tokens": 3000,
      "style": "academic_ko",
      "keep_math": true
    }
  },
  "papers": {
    "default_project_rel_path": "논문리뷰",
    "output_subdir": "papers"
  }
}
```

---

## 4) 구현 단계(실행 계획)

### Phase 1 (MVP) — “돌아가게 만들기”

* [ ] `/api/papers/upload` + pdf 저장
* [ ] marker 실행 → `marker.md` 생성
* [ ] 헤딩 기반 chunker + `chunks.jsonl`
* [ ] gemini 또는 openai로 chunk 번역 → `translated.md`
* [ ] viewer에서 결과 파일 링크 제공

### Phase 2 — “품질/안정화”

* [ ] job 상태/진행률 조회 API + UI progress
* [ ] provider 키 로테이션/재시도/쿼터 에러 처리
* [ ] 표/수식/이미지 경로 안정화
* [ ] 실패한 chunk만 재시도 기능

### Phase 3 — “드래그 Q&A 주석”

* [ ] viewer selection UI + 모달
* [ ] `/api/annotate/ask` (답변 생성)
* [ ] `/api/annotate/apply` (정확/완화 매칭 후 삽입)
* [ ] 중복 방지 마커/히스토리 백업

---

## 5) 실패/예외 처리 규칙(필수)

* marker 실패:

  * job status = `failed`, `job.json`에 stderr 저장
  * UI에 “원본 PDF 다운로드”는 항상 가능
* 번역 실패(특정 chunk):

  * 해당 chunk만 `failed_chunks`로 기록 후 다음 chunk 진행(전체 파이프라인 중단 방지)
  * 최종 `translated.md`에는 실패 chunk 위치에 placeholder 남김:

    * `> [번역 실패] chunk_id=...`
* 삽입 매칭 실패:

  * 파일 맨 끝에 “주석 수집 섹션(appendix)”을 만들고 거기에라도 누적(데이터 유실 방지)

---

## 6) 테스트 체크리스트

* [ ] 한글 파일명/경로에서 업로드/저장/다운로드 모두 정상
* [ ] 수식 포함 문서: 렌더/번역 후 MathJax 깨짐 없음
* [ ] 표 포함 문서: markdown table 유지
* [ ] 이미지 포함 문서: `/research/media/...` 경로로 정상 표시
* [ ] 동일 구간 주석 2회 삽입 시 중복 방지 마커로 방지/또는 구분 저장
* [ ] `.history` 백업 생성 확인

---

## 7) 메모(코드 작성 시 준수)

* LLM 호출/marker/chunking 모두 **step별 logging** 필수
* 대량 chunk 처리 루프는 **tqdm**로 진행률 로그 출력(서버 콘솔/로그에 남게)
* prompt/response는 반드시 파일로 남겨서 “재현/디버깅” 가능하게 유지
